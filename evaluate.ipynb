{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer, Input, Dropout, Conv2D, Activation, add, UpSampling2D,     Conv2DTranspose, Flatten, Reshape\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization, InputSpec\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import os\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from skimage.transform import resize\n",
    "from skimage import color\n",
    "from helper_funcs import *\n",
    "from attention_module import attach_attention_module\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# ### Model parameters\n",
    "# \n",
    "# This CycleGAN implementation allows a lot of freedom on both the training parameters and the network architecture.\n",
    "\n",
    "opt = {}\n",
    "\n",
    "# Data\n",
    "opt['channels'] = 1\n",
    "opt['img_shape'] = (256,256,1)\n",
    "\n",
    "\n",
    "\n",
    "# #### Training parameters\n",
    "# - `lambda_ABA` and `lambda_BAB` set the importance of the cycle consistency losses in relation to the adversarial loss `lambda_adversarial`\n",
    "# - `learning_rate_D` and `learning_rate_G` are the learning rates for the discriminators and generators respectively.\n",
    "# - `generator_iterations` and `discriminator_iterations` represent how many times the generators or discriminators will be trained on every batch of images. This is very useful to keep the training of both systems balanced. In this case the discriminators become successful faster than the generators, so we account for this by training the generators 3 times on every batch of images.\n",
    "# - `synthetic_pool_size` sets the size of the image pool used for training the discriminators. The image pool has a certain probability of returning a synthetic image from previous iterations, thus forcing the discriminator to have a certain \"memory\". More information on this method can be found in [this paper](https://arxiv.org/abs/1612.07828).\n",
    "# - `beta_1` and `beta_2` are paremeters of the [Adam](https://arxiv.org/abs/1412.6980) optimizers used on the generators and discriminators.\n",
    "# - `batch_size` determines the number of images used for each update of the network weights. Due to the significant memory requirements of CycleGAN it is difficult to use a large batch size. For the small example dataset values between 1-30 may be possible.\n",
    "# - `epochs` sets the number of training epochs. Each epoch goes through all the training images once. The number of epochs necessary to train a model is therefore dependent on both the number of training images available and the batch size.\n",
    "\n",
    "# Training parameters\n",
    "opt['lambda_ABA'] = 10.0  # Cyclic loss weight A_2_B\n",
    "opt['lambda_BAB'] = 10.0  # Cyclic loss weight B_2_A\n",
    "opt['lambda_adversarial'] = 1.0  # Weight for loss from discriminator guess on synthetic images\n",
    "opt['learning_rate_D'] = 2e-4\n",
    "opt['learning_rate_G'] = 2e-4\n",
    "opt['generator_iterations'] = 3  # Number of generator training iterations in each training loop\n",
    "opt['discriminator_iterations'] = 1  # Number of discriminator training iterations in each training loop\n",
    "opt['synthetic_pool_size'] = 50  # Size of image pools used for training the discriminators\n",
    "opt['beta_1'] = 0.5  # Adam parameter\n",
    "opt['beta_2'] = 0.999  # Adam parameter\n",
    "opt['batch_size'] = 1  # Number of images per batch\n",
    "opt['epochs'] = 10  # Choose multiples of 20 since the models are saved each 20th epoch\n",
    "\n",
    "\n",
    "# Output parameters\n",
    "opt['save_models'] = True  # Save or not the generator and discriminator models\n",
    "opt['save_training_img'] = True  # Save or not example training results or only tmp.png\n",
    "opt['save_training_img_interval'] = 1  # Number of epoch between saves of intermediate training results\n",
    "opt['self.tmp_img_update_frequency'] = 3  # Number of batches between updates of tmp.png\n",
    "\n",
    "\n",
    "# #### Architecture parameters\n",
    "# - `use_instance_normalization` is supposed to allow the selection of instance normalization or batch normalization layes. At the moment only instance normalization is implemented, so this option does not do anything.\n",
    "# - `use_dropout` and `use_bias` allows setting droupout layers in the generators and whether to use a bias term in the various convolutional layer in the genrators and discriminators.\n",
    "# - `use_linear_decay` applies linear decay on the learning rates of the generators and discriminators,   `decay_epoch`\n",
    "# - `use_patchgan` determines whether the discriminator evaluates the \"realness\" of images on a patch basis or on the whole. More information on PatchGAN can be found in [this paper](https://arxiv.org/abs/1611.07004).\n",
    "# - `use_resize_convolution` provides two ways to perfrom the upsampling in the generator, with significant differences in the results. More information can be found in [this article](https://distill.pub/2016/deconv-checkerboard/). Each has its advantages, and we have managed to get successful result with both methods\n",
    "# - `use_discriminator sigmoid` adds a sigmoid activation at the end of the discrimintator, forcing its output to the (0-1) range.\n",
    "\n",
    "# Architecture parameters\n",
    "opt['use_instance_normalization'] = True  # Use instance normalization or batch normalization\n",
    "opt['use_dropout'] = False  # Dropout in residual blocks\n",
    "opt['use_bias'] = True  # Use bias\n",
    "opt['use_linear_decay'] = True  # Linear decay of learning rate, for both discriminators and generators\n",
    "opt['decay_epoch'] = 101  # The epoch where the linear decay of the learning rates start\n",
    "opt['use_patchgan'] = True  # PatchGAN - if false the discriminator learning rate should be decreased\n",
    "opt['use_resize_convolution'] = True  # Resize convolution - instead of transpose convolution in deconvolution layers (uk) - can reduce checkerboard artifacts but the blurring might affect the cycle-consistency\n",
    "opt['discriminator_sigmoid'] = False  # Add a final sigmoid activation to the discriminator\n",
    "\n",
    "\n",
    "# Tweaks\n",
    "opt['REAL_LABEL'] = 1.0  # Use e.g. 0.9 to avoid training the discriminators to zero loss\n",
    "\n",
    "\n",
    "# ### Model architecture\n",
    "# \n",
    "# #### Layer blocks\n",
    "# These are the individual layer blocks that are used to build the generators and discriminator. More information can be found in the appendix of the [CycleGAN paper](https://arxiv.org/abs/1703.10593).\n",
    "\n",
    "# Discriminator layers\n",
    "def ck(model, opt, x, k, use_normalization, use_bias):\n",
    "    x = Conv2D(filters=k, kernel_size=4, strides=2, padding='same', use_bias=use_bias)(x)\n",
    "    if use_normalization:\n",
    "        x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "# First generator layer\n",
    "def c7Ak(model, opt, x, k):\n",
    "    x = Conv2D(filters=k, kernel_size=7, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
    "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# Downsampling\n",
    "def dk(model, opt, x, k):  # Should have reflection padding\n",
    "    x = Conv2D(filters=k, kernel_size=3, strides=2, padding='same', use_bias=opt['use_bias'])(x)\n",
    "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# Residual block\n",
    "def Rk(model, opt, x0):\n",
    "    k = int(x0.shape[-1])\n",
    "\n",
    "    # First layer\n",
    "    x = ReflectionPadding2D((1,1))(x0)\n",
    "    x = Conv2D(filters=k, kernel_size=3, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
    "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    if opt['use_dropout']:\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "    # Second layer\n",
    "    x = ReflectionPadding2D((1, 1))(x)\n",
    "    x = Conv2D(filters=k, kernel_size=3, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
    "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "\n",
    "    x = attach_attention_module(x, 'cbam_block')\n",
    "    # Merge\n",
    "    x = add([x, x0])\n",
    "\n",
    "    return x\n",
    "\n",
    "def original_Rk(model, opt, x0):\n",
    "    k = int(x0.shape[-1])\n",
    "\n",
    "    # First layer\n",
    "    x = ReflectionPadding2D((1,1))(x0)\n",
    "    x = Conv2D(filters=k, kernel_size=3, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
    "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    if opt['use_dropout']:\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "    # Second layer\n",
    "    x = ReflectionPadding2D((1, 1))(x)\n",
    "    x = Conv2D(filters=k, kernel_size=3, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
    "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "\n",
    "    # Merge\n",
    "    x = add([x, x0])\n",
    "\n",
    "    return x\n",
    "\n",
    "# Upsampling\n",
    "def uk(model, opt, x, k):\n",
    "    # (up sampling followed by 1x1 convolution <=> fractional-strided 1/2)\n",
    "    if opt['use_resize_convolution']:\n",
    "        x = UpSampling2D(size=(2, 2))(x)  # Nearest neighbor upsampling\n",
    "        x = ReflectionPadding2D((1, 1))(x)\n",
    "        x = Conv2D(filters=k, kernel_size=3, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
    "    else:\n",
    "        x = Conv2DTranspose(filters=k, kernel_size=3, strides=2, padding='same', use_bias=opt['use_bias'])(x)  # this matches fractionally stided with stride 1/2\n",
    "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "# #### Architecture functions\n",
    "\n",
    "\n",
    "def build_generator(model, opt, name=None):\n",
    "    # Layer 1: Input\n",
    "\n",
    "    input_img = Input(shape=opt['img_shape'])\n",
    "    x = ReflectionPadding2D((3, 3))(input_img)\n",
    "    x = c7Ak(model, opt, x, 32)\n",
    "\n",
    "    # Layer 2-3: Downsampling\n",
    "    x = dk(model, opt, x, 64)\n",
    "    x = dk(model, opt, x, 128)\n",
    "\n",
    "#     x = SelfAttention(128)(x)\n",
    "    \n",
    "    # Layers 4-12: Residual blocks\n",
    "    for number_loop in range(4, 12):\n",
    "        x = Rk(model, opt, x)\n",
    "    x = original_Rk(model, opt, x)\n",
    "#         if number_loop in [4,5,6,7,8,9,10,11,12]:\n",
    "#             x = SelfAttention(128)(x)\n",
    "#     x = SelfAttention(128)(x)\n",
    "    # Layer 13:14: Upsampling\n",
    "    x = uk(model, opt, x, 64)\n",
    "    x = uk(model, opt, x, 32)\n",
    "\n",
    "    # Layer 15: Output\n",
    "    x = ReflectionPadding2D((3, 3))(x)\n",
    "    x = Conv2D(opt['channels'], kernel_size=7, strides=1, padding='valid', use_bias=True)(x)\n",
    "    x = Activation('tanh')(x)\n",
    "    # x = Reshape((217,181,1))(x)\n",
    "    # print(\"Generator Model:\")\n",
    "    # print(Model(inputs=input_img, outputs=x, name=name).summary())\n",
    "    return Model(inputs=input_img, outputs=x, name=name)\n",
    "\n",
    "\n",
    "# #### Loss functions\n",
    "# The discriminators use MSE loss. The generators use MSE for the adversarial losses and MAE for the cycle consistency losses.\n",
    "\n",
    "# Mean squared error\n",
    "def mse(y_true, y_pred):\n",
    "    loss = tf.reduce_mean(tf.squared_difference(y_pred, y_true))\n",
    "    return loss\n",
    "\n",
    "# Mean absolute error\n",
    "def mae(y_true, y_pred):\n",
    "    loss = tf.reduce_mean(tf.abs(y_pred - y_true))\n",
    "    return loss\n",
    "\n",
    "def celoss(y_true, y_pred):\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n",
    "    return loss\n",
    "\n",
    "#save and join\n",
    "def join_and_save(opt, images, save_path):\n",
    "    # Join images\n",
    "    image = np.hstack(images)\n",
    "\n",
    "    # Save images\n",
    "    if opt['channels'] == 1:\n",
    "        image = image[:, :, 0]\n",
    "        \n",
    "    mpimg.imsave(save_path, image, vmin=-1, vmax=1, cmap='gray')\n",
    "# Load Model\n",
    "\n",
    "model = {}\n",
    "# Normalization\n",
    "model['normalization'] = InstanceNormalization\n",
    "model['G_A2B'] = build_generator(model, opt, name='G_A2B_model')\n",
    "model['G_B2A'] = build_generator(model, opt, name='G_B2A_model')\n",
    "# Don't pre-allocate GPU memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd9a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "weight_path = '/home/jiayuan/ADC-cycleGAN/result/model' # Only need to change once, please change to your model root path.\n",
    "save_path = '/home/jiayuan/ADC-cycleGAN/result/generated_image'# Only need to change once, please change the path where you want to save the generated images.\n",
    "dataset_path = '/home/jiayuan/ADC-cycleGAN/dataset/cluster1/cluster4/2' # This is the same with your train command's dataset_path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b359ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "GA2B = model['G_A2B']\n",
    "for file_name in glob.iglob(weight_path+'/**'):\n",
    "    name = file_name.split('/')[-1]\n",
    "    datasets_path = os.path.join(dataset_path+name.split('loop')[1],'cluster'+name.split('loop')[0][-2],name.split('loop')[-1])\n",
    "    weight_file = glob.glob(os.path.join(weight_path,name)+'/G_A2B**.hdf5')\n",
    "    GA2B.load_weights(weight_file[0])\n",
    "\n",
    "    for image in glob.iglob(dataset_path+'/testCT/**'):\n",
    "        png_name=image.split('/')[-1]\n",
    "        image = mpimg.imread(image)\n",
    "        image = resize(image,(256,256))\n",
    "        image = image[:, :, np.newaxis]\n",
    "        image = image * 2 - 1\n",
    "        real_image = image\n",
    "        image = np.reshape(image,(1, 256,256,1))\n",
    "        real_another = mpimg.imread(os.path.join(dataset_path,'testMRI',png_name))\n",
    "        real_another = resize(real_another,(256,256))\n",
    "        real_another = real_another[:, :, np.newaxis]\n",
    "        real_another = real_another * 2 - 1\n",
    "        im = GA2B.predict(image)\n",
    "        im = np.reshape(im,(256,256))\n",
    "        im = im[:, :, np.newaxis]\n",
    "        save_image_path = os.path.join(save_path,name,'a2b',png_name)\n",
    "        if not os.path.exists(os.path.join(save_path,name,'a2b')):\n",
    "            os.makedirs(os.path.join(save_path,name,'a2b'))\n",
    "        join_and_save(opt, (real_image, im, real_another), save_image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc5b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "GB2A = model['G_B2A']\n",
    "for file_name in glob.iglob(weight_path+'/**'):\n",
    "    name = file_name.split('/')[-1]\n",
    "    datasets_path = os.path.join(dataset_path+name.split('loop')[1],'cluster'+name.split('loop')[0][-2],name.split('loop')[-1])\n",
    "    weight_file = glob.glob(os.path.join(weight_path,name)+'/G_B2A**.hdf5')\n",
    "    GB2A.load_weights(weight_file[0])\n",
    "\n",
    "    for image in glob.iglob(dataset_path+'/testMRI/**'):\n",
    "        png_name=image.split('/')[-1]\n",
    "        image = mpimg.imread(image)\n",
    "        image = resize(image,(256,256))\n",
    "        image = image[:, :, np.newaxis]\n",
    "        image = image * 2 - 1\n",
    "        real_image = image\n",
    "        image = np.reshape(image,(1, 256,256,1))\n",
    "        real_another = mpimg.imread(os.path.join(dataset_path,'testCT',png_name))\n",
    "        real_another = resize(real_another,(256,256))\n",
    "        real_another = real_another[:, :, np.newaxis]\n",
    "        real_another = real_another * 2 - 1\n",
    "        im = GB2A.predict(image)\n",
    "        im = np.reshape(im,(256,256))\n",
    "        im = im[:, :, np.newaxis]\n",
    "        save_image_path = os.path.join(save_path,name,'b2a',png_name)\n",
    "        if not os.path.exists(os.path.join(save_path,name,'b2a')):\n",
    "            os.makedirs(os.path.join(save_path,name,'b2a'))\n",
    "        join_and_save(opt, (real_image, im, real_another), save_image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99e9163",
   "metadata": {},
   "source": [
    "# Calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import compare_ssim, compare_psnr\n",
    "from scipy.misc import imread\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('evaluate.txt'):\n",
    "    os.remove('evaluate.txt')\n",
    "    \n",
    "for file_name in glob.iglob(os.path.join(save_path,'**/**')):\n",
    "    with open('evaluate.txt','a') as f:\n",
    "        ssim_amount = []\n",
    "        mae_amount = []\n",
    "        psnr_amount = []\n",
    "        worse_list = []\n",
    "        for image in glob.iglob(os.path.join(file_name,'**.png')):\n",
    "            img = mpimg.imread(image)\n",
    "            img = img[:,:,0]\n",
    "            img = img*2-1\n",
    "            img1 = img[:,256:512]\n",
    "            img2 = img[:,512:]\n",
    "            mae = mean_absolute_error(img2, img1)\n",
    "            psnr = compare_psnr(img2,img1)\n",
    "            ssim = compare_ssim(img1, img2, multichannel=True)\n",
    "            ssim_amount.append(ssim)\n",
    "            mae_amount.append(mae)\n",
    "            psnr_amount.append(psnr)\n",
    "        average_ssim = sum(ssim_amount)/len(ssim_amount)\n",
    "        average_mae = sum(mae_amount)/len(mae_amount)\n",
    "        average_psnr = sum(psnr_amount)/len(psnr_amount)\n",
    "\n",
    "        writes = image.split('/')[-3]+'_'+image.split('/')[-2]+': '+ str(average_mae)+','+ str(average_psnr)+','+ str(average_ssim)\n",
    "        f.write(writes+'\\n')\n",
    "        print('File: {}. {}. average_mae is {}, average_psnr is {}, average_ssim is {} '.format(image.split('/')[-3],image.split('/')[-2],str(average_mae),str(average_psnr),str(average_ssim)))\n",
    "    fopen=open(\"evaluate.txt\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81670c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MRCT]",
   "language": "python",
   "name": "conda-env-MRCT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
